apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  annotations:
    networkoperator.openshift.io/ignore-errors: ""
  name: networking-rules
  namespace: openshift-sdn
spec:
  groups:
  - name: cluster-network-operator-sdn.rules
    rules:
    # note: all joins on kube_pod_* need a a "topk by (key) (1, <metric> )"
    # otherwise you will generate query errors when kube_state_metrics is being
    # upgraded and there are duplicate rows on the "right" side of the join.
    - alert: NodeWithoutSDNController
      annotations:
        summary: All control plane nodes should be running an OpenShift SDN controller pod, {{"{{"}} $labels.node {{"}}"}} is not.
        description: |
          If at least one OpenShift SDN controller is 'Running', network control plane should be functional but
          high availability is degraded when a controller is not functional.
      expr: |
          count(kube_node_role{role="master"} == 1) != count(kube_pod_info{namespace="openshift-sdn",  pod=~"sdn-controller.*"})
      for: 10m
      labels:
        severity: warning
    - alert: NodeWithoutSDNPod
      annotations:
        summary: All nodes should be running an OpenShift SDN pod, {{"{{"}} $labels.node {{"}}"}} is not.
        description: Network control plane configuration on the node could be degraded.
      expr: |
        (kube_node_info unless on(node) topk by (node) (1, kube_pod_info{namespace="openshift-sdn",  pod=~"sdn-[^-]*"})) > 0
      for: 10m
      labels:
        severity: warning
    - alert: NodeProxyApplySlow
      annotations:
        summary: OpenShift SDN pod {{"{{"}} $labels.pod {{"}}"}} on node {{"{{"}} $labels.node {{"}}"}} is taking too long to update proxy rules for services.
        description: Configuration of proxy rules for Kubernetes services in the node is taking too long and stale endpoints may exist.
      expr: |
        histogram_quantile(.95, kubeproxy_sync_proxy_rules_duration_seconds_bucket)
        * on(namespace, pod) group_right topk by (namespace, pod) (1, kube_pod_info{namespace="openshift-sdn",  pod=~"sdn-[^-]*"}) > 15
      labels:
        severity: warning
    - alert: ClusterProxyApplySlow
      annotations:
        summary: The cluster is taking too long, on average, to apply kubernetes service rules to iptables.
        description: Configuration of proxy rules for Kubernetes services in the cluster is taking too long and stale endpoints may exist.
      expr: |
        histogram_quantile(0.95, sum(rate(kubeproxy_sync_proxy_rules_duration_seconds_bucket[5m])) by (le)) > 10
      labels:
        namespace: openshift-sdn
        severity: warning
    - alert: NodeProxyApplyStale
      annotations:
        summary: OpenShift SDN pod {{"{{"}} $labels.pod {{"}}"}} on node {{"{{"}} $labels.node {{"}}"}} has stale Kubernetes service rules.
        description: Stale proxy rules for Kubernetes services may increase the time to configure the network and may degrade the network.
      # Query: find pods where
      #  - the queued-timestamp is at least 30 seconds after the applied-timestamp
      #  - the pod (still) exists
      expr: |
        (kubeproxy_sync_proxy_rules_last_queued_timestamp_seconds - kubeproxy_sync_proxy_rules_last_timestamp_seconds)
        * on(namespace, pod) group_right() topk by (namespace, pod) (1, kube_pod_info{namespace="openshift-sdn",pod=~"sdn-[^-]*"})
        > 30
      for: 5m # quiet any churn on sdn restarts.
      labels:
        severity: warning
    - alert: SDNPodNotReady
      annotations:
        summary: OpenShift SDN pod {{"{{"}} $labels.pod {{"}}"}} on node {{"{{"}} $labels.node {{"}}"}} is not ready.
        description: Network control plane configuration on the node could be degraded.
      expr: |
        kube_pod_status_ready{namespace='openshift-sdn', condition='true'} == 0
      for: 10m
      labels:
        severity: warning
