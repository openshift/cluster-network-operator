apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  annotations:
    networkoperator.openshift.io/ignore-errors: ""
  name: networking-rules
  namespace: openshift-sdn
spec:
  groups:
  - name: general.rules
    rules:
    - alert: NodeWithoutOVSPod
      annotations:
        message: |
          All nodes should be running an ovs pod, {{"{{"}} $labels.node {{"}}"}} is not.
      expr: |
        (kube_node_info unless on(node) kube_pod_info{namespace="openshift-sdn",  pod=~"ovs.*"}) > 0
      for: 20m
      labels:
        severity: warning
    - alert: NodeWithoutSDNPod
      annotations:
        message: |
          All nodes should be running an sdn pod, {{"{{"}} $labels.node {{"}}"}} is not.
      expr: |
        (kube_node_info unless on(node) kube_pod_info{namespace="openshift-sdn",  pod=~"sdn.*"}) > 0
      for: 20m
      labels:
        severity: warning
    - alert: NetworkPodsCrashLooping
      annotations:
        message: Pod {{"{{"}} $labels.namespace{{"}}"}}/{{"{{"}} $labels.pod{{"}}"}} ({{"{{"}} $labels.container
          {{"}}"}}) is restarting {{"{{"}} printf "%.2f" $value {{"}}"}} times / 5 minutes.
      expr: |
        rate(kube_pod_container_status_restarts_total{namespace="openshift-sdn"}[15m]) * 60 * 5 > 0
      for: 1h
      labels:
        severity: warning
    - alert: IPTableSyncSDNPod
      annotations:
        message: SDN pod {{"{{"}} $labels.pod {{"}}"}} on node {{"{{"}} $labels.node {{"}}"}} takes too long to sync iptables rules.
      expr: |
        histogram_quantile(.95, kubeproxy_sync_proxy_rules_duration_seconds_bucket) * on(pod) group_right kube_pod_info{namespace="openshift-sdn",  pod=~"sdn-[^-]*"} > 15
      labels:
        severity: warning
    - alert: IPTableSyncCluster
      annotations:
        message: The average time for SDN pods to sync iptables is too high.
      expr: |
        histogram_quantile(0.95, sum(rate(kubeproxy_sync_proxy_rules_duration_seconds_bucket[5m])) by (le)) > 10
      labels:
        severity: warning
    - alert: NodeIPTablesStale
    # there is some scrape delay and some other offset 120 is not really 120s but it is still too long
      annotations:
        message: SDN pod {{"{{"}} $labels.pod {{"}}"}} on node {{"{{"}} $labels.node {{"}}"}} has gone too long without syncing iptables rules.
      expr: |
            (timestamp(kubeproxy_sync_proxy_rules_last_timestamp_seconds)
            - on(pod) kubeproxy_sync_proxy_rules_last_timestamp_seconds)
            * on(pod) group_right kube_pod_info{namespace="openshift-sdn",  pod=~"sdn-[^-]*"}> 120
      for: 20m
      labels:
        severity: warning
    - alert: ClusterIPTablesStale
    # there is some scrape delay and some other offset 90 i not really 90s but it is still too long
      annotations:
        message: The average time between iptables resyncs is too high. NOTE - There is some scrape delay and other offsets, 90s isn't exact but it is still too high.
      expr: |
        quantile(0.95,
            timestamp(kubeproxy_sync_proxy_rules_last_timestamp_seconds)
            - on(pod) kubeproxy_sync_proxy_rules_last_timestamp_seconds
            * on(pod) group_right kube_pod_info{namespace="openshift-sdn",  pod=~"sdn-[^-]*"}) > 90
      for: 20m
      labels:
        severity: warning
