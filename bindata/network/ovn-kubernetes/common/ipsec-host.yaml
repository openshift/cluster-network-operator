{{if .OVNIPsecDaemonsetEnable}}
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: ovn-ipsec-host
  namespace: openshift-ovn-kubernetes
  annotations:
    kubernetes.io/description: |
      This DaemonSet launches the ovn ipsec networking components for all nodes.
    release.openshift.io/version: "{{.ReleaseVersion}}"
spec:
  selector:
    matchLabels:
      app: ovn-ipsec
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 10%
  template:
    metadata:
      annotations:
        target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        # prevent blocks when node critical pods get evicted prior to workloads
        cluster-autoscaler.kubernetes.io/enable-ds-eviction: "false"
      labels:
        app: ovn-ipsec
        component: network
        type: infra
        openshift.io/component: network
        kubernetes.io/os: "linux"
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: network.operator.openshift.io/dpu-host
                operator: DoesNotExist
      serviceAccountName: ovn-kubernetes-node
      hostPID: true
      hostNetwork: true
      dnsPolicy: Default
      priorityClassName: "system-node-critical"
      initContainers:
      - name: ovn-keys
        image: "{{.OvnImage}}"
        command:
        - /bin/bash
        - -c
        - |
          #!/bin/bash
          set -exuo pipefail
{{ if .IPsecServiceCheckOnHost }}
          if ! chroot /proc/1/root systemctl is-active --quiet ipsec.service; then
            echo "host doesn't have ipsec.service running, therefore ipsec will be configured by ipsec-containerized daemonset, this ovn ipsec container has nothing to init"
            exit 0
          fi
{{ end }}
{{ if .NETWORK_NODE_IDENTITY_ENABLE }}
          # When NETWORK_NODE_IDENTITY_ENABLE is true, use the per-node certificate to create a kubeconfig
          # that will be used to talk to the API


          # Wait for cert file
          retries=0
          tries=20
          key_cert="/etc/ovn/ovnkube-node-certs/ovnkube-client-current.pem"
          while [ ! -f "${key_cert}" ]; do
            (( retries += 1 ))
            if [[ "${retries}" -gt ${tries} ]]; then
              echo "$(date -Iseconds) - ERROR - ${key_cert} not found"
              return 1
            fi
            sleep 1
          done

          cat << EOF > /var/run/ovnkube-kubeconfig
          apiVersion: v1
          clusters:
            - cluster:
                certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                server: {{.K8S_APISERVER}}
              name: default-cluster
          contexts:
            - context:
                cluster: default-cluster
                namespace: default
                user: default-auth
              name: default-context
          current-context: default-context
          kind: Config
          preferences: {}
          users:
            - name: default-auth
              user:
                client-certificate: /etc/ovn/ovnkube-node-certs/ovnkube-client-current.pem
                client-key: /etc/ovn/ovnkube-node-certs/ovnkube-client-current.pem
          EOF
          export KUBECONFIG=/var/run/ovnkube-kubeconfig
{{ end }}

          # Every time we restart this container, we will create a new key pair if
          # we are close to key expiration or if we do not already have a signed key pair.
          #
          # Each node has a key pair which is used by OVS to encrypt/decrypt/authenticate traffic
          # between each node. The CA cert is used as the root of trust for all certs so we need
          # the CA to sign our certificate signing requests with the CA private key. In this way,
          # we can validate that any signed certificates that we receive from other nodes are
          # authentic.
          echo "Configuring IPsec keys"

          cert_pem=/etc/openvswitch/keys/ipsec-cert.pem

          # If the certificate does not exist or it will expire in the next 6 months
          # (15770000 seconds), we will generate a new one.
          if ! openssl x509 -noout -dates -checkend 15770000 -in $cert_pem; then
            # We use the system-id as the CN for our certificate signing request. This
            # is a requirement by OVN.
            cn=$(ovs-vsctl --retry -t 60 get Open_vSwitch . external-ids:system-id | tr -d "\"")

            mkdir -p /etc/openvswitch/keys

            # Generate an SSL private key and use the key to create a certitificate signing request
            umask 077 && openssl genrsa -out /etc/openvswitch/keys/ipsec-privkey.pem 2048
            openssl req -new -text \
                        -extensions v3_req \
                        -addext "subjectAltName = DNS:${cn}" \
                        -subj "/C=US/O=ovnkubernetes/OU=kind/CN=${cn}" \
                        -key /etc/openvswitch/keys/ipsec-privkey.pem \
                        -out /etc/openvswitch/keys/ipsec-req.pem

            csr_64=$(base64 -w0 /etc/openvswitch/keys/ipsec-req.pem) # -w0 to avoid line-wrap

            # Request that our generated certificate signing request is
            # signed by the "network.openshift.io/signer" signer that is
            # implemented by the CNO signer controller. This will sign the
            # certificate signing request using the signer-ca which has been
            # set up by the OperatorPKI. In this way, we have a signed certificate
            # and our private key has remained private on this host.
            cat <<EOF | kubectl create -f -
            apiVersion: certificates.k8s.io/v1
            kind: CertificateSigningRequest
            metadata:
              generateName: ipsec-csr-$(hostname)-
              labels:
                k8s.ovn.org/ipsec-csr: $(hostname)
            spec:
              request: ${csr_64}
              signerName: network.openshift.io/signer
              usages:
              - ipsec tunnel
          EOF
            # Wait until the certificate signing request has been signed.
            counter=0
            until [ -n "$(kubectl get csr -lk8s.ovn.org/ipsec-csr="$(hostname)" --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-1:].status.certificate}' 2>/dev/null)" ]
            do
              counter=$((counter+1))
              sleep 1
              if [ $counter -gt 60 ];
              then
                      echo "Unable to sign certificate after $counter seconds"
                      exit 1
              fi
            done

            # Decode the signed certificate.
            kubectl get csr -lk8s.ovn.org/ipsec-csr="$(hostname)" --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-1:].status.certificate}' | base64 -d | openssl x509 -outform pem -text -out $cert_pem

            # kubectl delete csr/$(hostname)

            # Get the CA certificate so we can authenticate peer nodes.
            openssl x509 -in /signer-ca/ca-bundle.crt -outform pem -text -out /etc/openvswitch/keys/ipsec-cacert.pem
          fi

          # Configure OVS with the relevant keys for this node. This is required by ovs-monitor-ipsec.
          #
          # Updating the certificates does not need to be an atomic operation as
          # the will get read and loaded into NSS by the ovs-monitor-ipsec process
          # which has not started yet.
          ovs-vsctl --retry -t 60 set Open_vSwitch . other_config:certificate=$cert_pem \
                                                     other_config:private_key=/etc/openvswitch/keys/ipsec-privkey.pem \
                                                     other_config:ca_cert=/etc/openvswitch/keys/ipsec-cacert.pem
        env:
        - name: K8S_NODE
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        securityContext:
          privileged: true
        volumeMounts:
{{ if .NETWORK_NODE_IDENTITY_ENABLE }}
        - mountPath: /etc/ovn/
          name: etc-ovn
{{ end }}
        - mountPath: /var/run
          name: host-var-run
        - mountPath: /signer-ca
          name: signer-ca
        - mountPath: /etc/openvswitch
          name: etc-openvswitch
        - mountPath: /etc
          name: host-etc
        resources:
          requests:
            cpu: 10m
            memory: 100Mi
        terminationMessagePolicy: FallbackToLogsOnError
      containers:
      # ovs-monitor-ipsec and libreswan daemons
      - name: ovn-ipsec
        image: "{{.OvnImage}}"
        command:
        - /bin/bash
        - -c
        - |
          #!/bin/bash
          set -exuo pipefail

{{ if .IPsecServiceCheckOnHost }}
          if ! chroot /proc/1/root systemctl is-active --quiet ipsec.service; then
            echo "host doesn't have ipsec.service running, therefore ipsec will be configured by ipsec-containerized daemonset, this ovn ipsec container will sleep to infinity"
            sleep infinity
          fi
{{ end }}

          # Don't start IPsec until ovnkube-node has finished setting up the node
          counter=0
          until [ -f /etc/cni/net.d/10-ovn-kubernetes.conf ]
          do
            counter=$((counter+1))
            sleep 1
            if [ $counter -gt 300 ];
            then
                    echo "ovnkube-node pod has not started after $counter seconds"
                    exit 1
            fi
          done
          echo "ovnkube-node has configured node."

          if ! pgrep pluto; then
            echo "pluto is not running, enable the service and/or check system logs"
            exit 2
          fi

          # The ovs-monitor-ipsec doesn't set authby, so when it calls ipsec auto --start
          # the default ones defined at Libreswan's compile time will be used. On restart,
          # Libreswan will use authby from libreswan.config. If libreswan.config is
          # incompatible with the Libreswan's compiled-in defaults, then we'll have an
          # authentication problem. But OTOH, ovs-monitor-ipsec does set ike and esp algorithms,
          # so those may be incompatible with libreswan.config as well. Hence commenting out the
          # "include" from libreswan.conf to avoid such conflicts.
          defaultcpinclude="include \/etc\/crypto-policies\/back-ends\/libreswan.config"
          if ! grep -q "# ${defaultcpinclude}" /etc/ipsec.conf; then
            sed -i "/${defaultcpinclude}/s/^/# /" /etc/ipsec.conf
            # since pluto is on the host, we need to restart it after changing connection
            # parameters.
            chroot /proc/1/root ipsec restart

            counter=0
            until [ -r /run/pluto/pluto.ctl ]; do
              counter=$((counter+1))
              sleep 1
              if [ $counter -gt 300 ];
              then
                echo "ipsec has not started after $counter seconds"
                exit 1
              fi
            done
            echo "ipsec service is restarted"
          fi

          # Workaround for https://github.com/libreswan/libreswan/issues/373
          ulimit -n 1024

          /usr/libexec/ipsec/addconn --config /etc/ipsec.conf --checkconfig
          # Check kernel modules only for libreswan version <= 5.2. The _stackmanager binary is
          # removed from 5.3 onwards, so this check is not needed on later versions.
          if [ -e /usr/libexec/ipsec/_stackmanager ]; then
            /usr/libexec/ipsec/_stackmanager start
          fi
          # Check nss database status
          /usr/sbin/ipsec --checknss

          # Start ovs-monitor-ipsec which will monitor for changes in the ovs
          # tunnelling configuration (for example addition of a node) and configures
          # libreswan appropriately.
          # We are running this in the foreground so that the container will be restarted when ovs-monitor-ipsec fails.
          /usr/libexec/platform-python /usr/share/openvswitch/scripts/ovs-monitor-ipsec \
            --pidfile=/var/run/openvswitch/ovs-monitor-ipsec.pid --ike-daemon=libreswan --no-restart-ike-daemon \
            --ipsec-conf /etc/ipsec.d/openshift.conf --ipsec-d /var/lib/ipsec/nss \
            --log-file --monitor unix:/var/run/openvswitch/db.sock
        lifecycle:
           preStop:
             exec:
               command:
                 - /bin/bash
                 - -c
                 - |
                   #!/bin/bash
                   set -exuo pipefail
                   # In order to maintain traffic flows during container restart, we
                   # need to ensure that xfrm state and policies are not flushed.

{{ if .IPsecServiceCheckOnHost }}
                   if ! chroot /proc/1/root systemctl is-active --quiet ipsec.service; then
                     echo "host doesn't have ipsec.service running, therefore ipsec will be configured by ipsec-containerized daemonset, preStop wont do anything"
                     exit 0
                   fi
{{ end }}

                   # Don't allow ovs monitor to cleanup persistent state
                   kill "$(cat /var/run/openvswitch/ovs-monitor-ipsec.pid 2>/dev/null)" 2>/dev/null || true
        env:
        - name: K8S_NODE
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        securityContext:
          privileged: true
        volumeMounts:
        # To check that network setup is complete
        - mountPath: /etc/cni/net.d
          name: host-cni-netd
        - mountPath: /var/run
          name: host-var-run
        - mountPath: /var/log/openvswitch/
          name: host-var-log-ovs
        - mountPath: /etc/openvswitch
          name: etc-openvswitch
        - mountPath: /var/lib
          name: host-var-lib
        - mountPath: /etc
          name: host-etc
        - mountPath: /usr/sbin
          name: usr-sbin
        - mountPath: /usr/libexec
          name: usr-libexec
        resources:
          requests:
            cpu: 10m
            memory: 100Mi
        terminationMessagePolicy: FallbackToLogsOnError
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - |
              #!/bin/bash
{{ if .IPsecServiceCheckOnHost }}
              if ! chroot /proc/1/root systemctl is-active --quiet ipsec.service; then
                echo "host doesn't have ipsec.service running, therefore ipsec will be configured by ipsec-containerized daemonset, this ovn ipsec container is always \"alive\""
                exit 0
              fi
{{ end }}
              if [[ $(ipsec whack --trafficstatus | wc -l) -eq 0 ]]; then
                echo "no ipsec traffic configured"
                exit 10
              fi
          initialDelaySeconds: 15
          periodSeconds: 60
      - name: ovn-ipsec-cleanup
        image: "{{.OvnImage}}"
        command:
        - /bin/bash
        - -c
        - |
          #!/bin/bash
{{ if .NETWORK_NODE_IDENTITY_ENABLE }}
          # When NETWORK_NODE_IDENTITY_ENABLE is true, use the per-node certificate to create a kubeconfig
          # that will be used to talk to the API


          # Wait for cert file
          retries=0
          tries=20
          key_cert="/etc/ovn/ovnkube-node-certs/ovnkube-client-current.pem"
          while [ ! -f "${key_cert}" ]; do
            (( retries += 1 ))
            if [[ "${retries}" -gt ${tries} ]]; then
              echo "$(date -Iseconds) - ERROR - ${key_cert} not found"
              return 1
            fi
            sleep 1
          done

          cat << EOF > /var/run/ovnkube-kubeconfig
          apiVersion: v1
          clusters:
            - cluster:
                certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                server: {{.K8S_APISERVER}}
              name: default-cluster
          contexts:
            - context:
                cluster: default-cluster
                namespace: default
                user: default-auth
              name: default-context
          current-context: default-context
          kind: Config
          preferences: {}
          users:
            - name: default-auth
              user:
                client-certificate: /etc/ovn/ovnkube-node-certs/ovnkube-client-current.pem
                client-key: /etc/ovn/ovnkube-node-certs/ovnkube-client-current.pem
          EOF
          export KUBECONFIG=/var/run/ovnkube-kubeconfig
{{ end }}

          # It is safe to flush xfrm states and policies and delete openshift.conf
          # file when east-west ipsec is disabled. This fixes a race condition when
          # ovs-monitor-ipsec is not fast enough to notice ipsec config change and
          # delete entries before it's being killed.
          # Since it's cleaning up all xfrm states and policies, it may cause slight
          # interruption until ipsec is restarted in case of external ipsec config.
          # We must do this before killing ovs-monitor-ipsec script, otherwise
          # preStop hook doesn't get a chance to run it because ovn-ipsec container
          # is abruptly terminated.
          # When east-west ipsec is not disabled, then do not flush xfrm states and
          # policies in order to maintain traffic flows during container restart.
          ipsecflush() {
            if [ "$(kubectl get networks.operator.openshift.io cluster -ojsonpath='{.spec.defaultNetwork.ovnKubernetesConfig.ipsecConfig.mode}')" != "Full" ] && \
               [ "$(kubectl get networks.operator.openshift.io cluster -ojsonpath='{.spec.defaultNetwork.ovnKubernetesConfig.ipsecConfig}')" != "{}" ]; then
              ip x s flush
              ip x p flush
              rm -f /etc/ipsec.d/openshift.conf
              # since pluto is on the host, we need to restart it after the flush
              chroot /proc/1/root ipsec restart
            fi
          }

          # Function to handle SIGTERM
          cleanup() {
            echo "received SIGTERM, flushing ipsec config"
            # Wait upto 15 seconds for ovs-monitor-ipsec process to terminate before
            # cleaning up ipsec entries.
            counter=0
            while kill -0 "$(cat /var/run/openvswitch/ovs-monitor-ipsec.pid 2>/dev/null)"; do
              counter=$((counter+1))
              sleep 1
              if [ $counter -gt 15 ];
              then
                echo "ovs-monitor-ipsec has not terminated after $counter seconds"
                break
              fi
            done
            ipsecflush
            exit 0
          }

          # Trap SIGTERM and call cleanup function
          trap cleanup SIGTERM

          counter=0
          until [ -r /var/run/openvswitch/ovs-monitor-ipsec.pid ]; do
            counter=$((counter+1))
            sleep 1
            if [ $counter -gt 300 ];
            then
              echo "ovs-monitor-ipsec has not started after $counter seconds"
              exit 1
            fi
          done
          echo "ovs-monitor-ipsec is started"

          # Monitor the ovs-monitor-ipsec process.
          while kill -0 "$(cat /var/run/openvswitch/ovs-monitor-ipsec.pid 2>/dev/null)"; do
            sleep 1
          done

          # Once the ovs-monitor-ipsec process terminates, execute the cleanup command.
          echo "ovs-monitor-ipsec is terminated, flushing ipsec config"
          ipsecflush

          # Continue running until SIGTERM is received (or exit naturally)
          while true; do
            sleep 1
          done
        securityContext:
          privileged: true
        volumeMounts:
{{ if .NETWORK_NODE_IDENTITY_ENABLE }}
        - mountPath: /etc/ovn/
          name: etc-ovn
{{ end }}
        - mountPath: /var/run
          name: host-var-run
        - mountPath: /etc
          name: host-etc
        resources:
          requests:
            cpu: 10m
            memory: 50Mi
        terminationMessagePolicy: FallbackToLogsOnError
      nodeSelector:
        kubernetes.io/os: "linux"
      terminationGracePeriodSeconds: 10
      volumes:
{{ if .NETWORK_NODE_IDENTITY_ENABLE }}
      - name: etc-ovn
        hostPath:
          path: /var/lib/ovn-ic/etc
{{ end }}
      - hostPath:
          path: /var/log/openvswitch
          type: DirectoryOrCreate
        name: host-var-log-ovs
      - configMap:
          defaultMode: 420
          name: signer-ca
        name: signer-ca
      - hostPath:
          path: /var/lib/openvswitch/etc
          type: DirectoryOrCreate
        name: etc-openvswitch
      - hostPath:
          path: "{{.CNIConfDir}}"
        name: host-cni-netd
      - hostPath:
          path: /var/run
          type: DirectoryOrCreate
        name: host-var-run
      - hostPath:
          path: /var/lib
          type: DirectoryOrCreate
        name: host-var-lib
      - hostPath:
          path: /etc
          type: Directory
        name: host-etc
      - hostPath:
          path: /usr/sbin
          type: Directory
        name: usr-sbin
      - hostPath:
          path: /usr/libexec
          type: Directory
        name: usr-libexec
      tolerations:
      - operator: "Exists"
{{end}}
