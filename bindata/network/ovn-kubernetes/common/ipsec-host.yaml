{{if .OVNIPsecDaemonsetEnable}}
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: ovn-ipsec-host
  namespace: openshift-ovn-kubernetes
  annotations:
    kubernetes.io/description: |
      This DaemonSet launches the ovn ipsec networking components for all nodes.
    release.openshift.io/version: "{{.ReleaseVersion}}"
spec:
  selector:
    matchLabels:
      app: ovn-ipsec
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 10%
  template:
    metadata:
      annotations:
        target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        # prevent blocks when node critical pods get evicted prior to workloads
        cluster-autoscaler.kubernetes.io/enable-ds-eviction: "false"
      labels:
        app: ovn-ipsec
        component: network
        type: infra
        openshift.io/component: network
        kubernetes.io/os: "linux"
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: network.operator.openshift.io/dpu-host
                operator: DoesNotExist
      serviceAccountName: ovn-kubernetes-node
      hostPID: true
      hostNetwork: true
      dnsPolicy: Default
      priorityClassName: "system-node-critical"
      initContainers:
      - name: ovn-keys
        image: "{{.OvnImage}}"
        command:
        - /bin/bash
        - -c
        - |
          #!/bin/bash
          set -e
          . /ovnkube-lib/ovnkube-lib.sh || exit 1
          configure-ipsec-certkey-ovs
        env:
        - name: K8S_NODE
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        securityContext:
          privileged: true
        volumeMounts:
{{ if .NETWORK_NODE_IDENTITY_ENABLE }}
        - mountPath: /etc/ovn/
          name: etc-ovn
{{ end }}
        - mountPath: /var/run
          name: host-var-run
        - mountPath: /signer-ca
          name: signer-ca
        - mountPath: /etc/openvswitch
          name: etc-openvswitch
        - mountPath: /etc
          name: host-etc
        - mountPath: /ovnkube-lib
          name: ovnkube-script-lib
        resources:
          requests:
            cpu: 10m
            memory: 100Mi
        terminationMessagePolicy: FallbackToLogsOnError
      containers:
      # ovs-monitor-ipsec and libreswan daemons
      - name: ovn-ipsec
        image: "{{.OvnImage}}"
        command:
        - /bin/bash
        - -c
        - |
          #!/bin/bash
          set -exuo pipefail

          # Don't start IPsec until ovnkube-node has finished setting up the node
          counter=0
          until [ -f /etc/cni/net.d/10-ovn-kubernetes.conf ]
          do
            counter=$((counter+1))
            sleep 1
            if [ $counter -gt 300 ];
            then
                    echo "ovnkube-node pod has not started after $counter seconds"
                    exit 1
            fi
          done
          echo "ovnkube-node has configured node."

          if ! pgrep pluto; then
            echo "pluto is not running, enable the service and/or check system logs"
            exit 2
          fi

          # The ovs-monitor-ipsec doesn't set authby, so when it calls ipsec auto --start
          # the default ones defined at Libreswan's compile time will be used. On restart,
          # Libreswan will use authby from libreswan.config. If libreswan.config is
          # incompatible with the Libreswan's compiled-in defaults, then we'll have an
          # authentication problem. But OTOH, ovs-monitor-ipsec does set ike and esp algorithms,
          # so those may be incompatible with libreswan.config as well. Hence commenting out the
          # "include" from libreswan.conf to avoid such conflicts.
          defaultcpinclude="include \/etc\/crypto-policies\/back-ends\/libreswan.config"
          if ! grep -q "# ${defaultcpinclude}" /etc/ipsec.conf; then
            sed -i "/${defaultcpinclude}/s/^/# /" /etc/ipsec.conf
            # since pluto is on the host, we need to restart it after changing connection
            # parameters.
            chroot /proc/1/root ipsec restart

            counter=0
            until [ -r /run/pluto/pluto.ctl ]; do
              counter=$((counter+1))
              sleep 1
              if [ $counter -gt 300 ];
              then
                echo "ipsec has not started after $counter seconds"
                exit 1
              fi
            done
            echo "ipsec service is restarted"
          fi

          # Workaround for https://github.com/libreswan/libreswan/issues/373
          ulimit -n 1024

          /usr/libexec/ipsec/addconn --config /etc/ipsec.conf --checkconfig
          # Check kernel modules
          /usr/libexec/ipsec/_stackmanager start
          # Check nss database status
          /usr/sbin/ipsec --checknss

          # Start ovs-monitor-ipsec which will monitor for changes in the ovs
          # tunnelling configuration (for example addition of a node) and configures
          # libreswan appropriately.
          # We are running this in the foreground so that the container will be restarted when ovs-monitor-ipsec fails.
          /usr/libexec/platform-python /usr/share/openvswitch/scripts/ovs-monitor-ipsec \
            --pidfile=/var/run/openvswitch/ovs-monitor-ipsec.pid --ike-daemon=libreswan --no-restart-ike-daemon \
            --ipsec-conf /etc/ipsec.d/openshift.conf --ipsec-d /var/lib/ipsec/nss \
            --log-file --monitor unix:/var/run/openvswitch/db.sock
        lifecycle:
           preStop:
             exec:
               command:
                 - /bin/bash
                 - -c
                 - |
                   #!/bin/bash
                   set -exuo pipefail
                   # In order to maintain traffic flows during container restart, we
                   # need to ensure that xfrm state and policies are not flushed.

                   # Don't allow ovs monitor to cleanup persistent state
                   kill "$(cat /var/run/openvswitch/ovs-monitor-ipsec.pid 2>/dev/null)" 2>/dev/null || true
        env:
        - name: K8S_NODE
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        securityContext:
          privileged: true
        volumeMounts:
        # To check that network setup is complete
        - mountPath: /etc/cni/net.d
          name: host-cni-netd
        - mountPath: /var/run
          name: host-var-run
        - mountPath: /var/log/openvswitch/
          name: host-var-log-ovs
        - mountPath: /etc/openvswitch
          name: etc-openvswitch
        - mountPath: /var/lib
          name: host-var-lib
        - mountPath: /etc
          name: host-etc
        - mountPath: /usr/sbin/ipsec
          name: ipsec-bin
        - mountPath: /usr/libexec/ipsec
          name: ipsec-lib
        resources:
          requests:
            cpu: 10m
            memory: 100Mi
        terminationMessagePolicy: FallbackToLogsOnError
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - |
              #!/bin/bash
              if [[ $(ipsec whack --trafficstatus | wc -l) -eq 0 ]]; then
                echo "no ipsec traffic configured"
                exit 10
              fi
          initialDelaySeconds: 15
          periodSeconds: 60
      - name: ovn-ipsec-cleanup
        image: "{{.OvnImage}}"
        command:
        - /bin/bash
        - -c
        - |
          #!/bin/bash
{{ if .NETWORK_NODE_IDENTITY_ENABLE }}
          # When NETWORK_NODE_IDENTITY_ENABLE is true, use the per-node certificate to create a kubeconfig
          # that will be used to talk to the API


          # Wait for cert file
          retries=0
          tries=20
          key_cert="/etc/ovn/ovnkube-node-certs/ovnkube-client-current.pem"
          while [ ! -f "${key_cert}" ]; do
            (( retries += 1 ))
            if [[ "${retries}" -gt ${tries} ]]; then
              echo "$(date -Iseconds) - ERROR - ${key_cert} not found"
              return 1
            fi
            sleep 1
          done

          cat << EOF > /var/run/ovnkube-kubeconfig
          apiVersion: v1
          clusters:
            - cluster:
                certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                server: {{.K8S_APISERVER}}
              name: default-cluster
          contexts:
            - context:
                cluster: default-cluster
                namespace: default
                user: default-auth
              name: default-context
          current-context: default-context
          kind: Config
          preferences: {}
          users:
            - name: default-auth
              user:
                client-certificate: /etc/ovn/ovnkube-node-certs/ovnkube-client-current.pem
                client-key: /etc/ovn/ovnkube-node-certs/ovnkube-client-current.pem
          EOF
          export KUBECONFIG=/var/run/ovnkube-kubeconfig
{{ end }}

          # It is safe to flush xfrm states and policies and delete openshift.conf
          # file when east-west ipsec is disabled. This fixes a race condition when
          # ovs-monitor-ipsec is not fast enough to notice ipsec config change and
          # delete entries before it's being killed.
          # Since it's cleaning up all xfrm states and policies, it may cause slight
          # interruption until ipsec is restarted in case of external ipsec config.
          # We must do this before killing ovs-monitor-ipsec script, otherwise
          # preStop hook doesn't get a chance to run it because ovn-ipsec container
          # is abruptly terminated.
          # When east-west ipsec is not disabled, then do not flush xfrm states and
          # policies in order to maintain traffic flows during container restart.
          ipsecflush() {
            if [ "$(kubectl get networks.operator.openshift.io cluster -ojsonpath='{.spec.defaultNetwork.ovnKubernetesConfig.ipsecConfig.mode}')" != "Full" ] && \
               [ "$(kubectl get networks.operator.openshift.io cluster -ojsonpath='{.spec.defaultNetwork.ovnKubernetesConfig.ipsecConfig}')" != "{}" ]; then
              ip x s flush
              ip x p flush
              rm -f /etc/ipsec.d/openshift.conf
              # since pluto is on the host, we need to restart it after the flush
              chroot /proc/1/root ipsec restart
            fi
          }

          # Function to handle SIGTERM
          cleanup() {
            echo "received SIGTERM, flushing ipsec config"
            # Wait upto 15 seconds for ovs-monitor-ipsec process to terminate before
            # cleaning up ipsec entries.
            counter=0
            while kill -0 "$(cat /var/run/openvswitch/ovs-monitor-ipsec.pid 2>/dev/null)"; do
              counter=$((counter+1))
              sleep 1
              if [ $counter -gt 15 ];
              then
                echo "ovs-monitor-ipsec has not terminated after $counter seconds"
                break
              fi
            done
            ipsecflush
            exit 0
          }

          # Trap SIGTERM and call cleanup function
          trap cleanup SIGTERM

          counter=0
          until [ -r /var/run/openvswitch/ovs-monitor-ipsec.pid ]; do
            counter=$((counter+1))
            sleep 1
            if [ $counter -gt 300 ];
            then
              echo "ovs-monitor-ipsec has not started after $counter seconds"
              exit 1
            fi
          done
          echo "ovs-monitor-ipsec is started"

          # Monitor the ovs-monitor-ipsec process.
          while kill -0 "$(cat /var/run/openvswitch/ovs-monitor-ipsec.pid 2>/dev/null)"; do
            sleep 1
          done

          # Once the ovs-monitor-ipsec process terminates, execute the cleanup command.
          echo "ovs-monitor-ipsec is terminated, flushing ipsec config"
          ipsecflush

          # Continue running until SIGTERM is received (or exit naturally)
          while true; do
            sleep 1
          done
        securityContext:
          privileged: true
        volumeMounts:
{{ if .NETWORK_NODE_IDENTITY_ENABLE }}
        - mountPath: /etc/ovn/
          name: etc-ovn
{{ end }}
        - mountPath: /var/run
          name: host-var-run
        - mountPath: /etc
          name: host-etc
        resources:
          requests:
            cpu: 10m
            memory: 50Mi
        terminationMessagePolicy: FallbackToLogsOnError
      nodeSelector:
        kubernetes.io/os: "linux"
      terminationGracePeriodSeconds: 10
      volumes:
{{ if .NETWORK_NODE_IDENTITY_ENABLE }}
      - name: etc-ovn
        hostPath:
          path: /var/lib/ovn-ic/etc
{{ end }}
      - hostPath:
          path: /var/log/openvswitch
          type: DirectoryOrCreate
        name: host-var-log-ovs
      - configMap:
          defaultMode: 420
          name: signer-ca
        name: signer-ca
      - hostPath:
          path: /var/lib/openvswitch/etc
          type: DirectoryOrCreate
        name: etc-openvswitch
      - hostPath:
          path: "{{.CNIConfDir}}"
        name: host-cni-netd
      - hostPath:
          path: /var/run
          type: DirectoryOrCreate
        name: host-var-run
      - hostPath:
          path: /var/lib
          type: DirectoryOrCreate
        name: host-var-lib
      - hostPath:
          path: /etc
          type: Directory
        name: host-etc
      - hostPath:
          path: /usr/sbin/ipsec
          type: File
        name: ipsec-bin
      - hostPath:
          path: /usr/libexec/ipsec
          type: Directory
        name: ipsec-lib
      - name: ovnkube-script-lib
        configMap:
          name: ovnkube-script-lib
          defaultMode: 0744
      tolerations:
      - operator: "Exists"
{{end}}
