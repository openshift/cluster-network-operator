{{if .OVNIPsecDaemonsetEnable}}
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: ovn-ipsec-containerized
  namespace: openshift-ovn-kubernetes
  annotations:
    kubernetes.io/description: |
      This DaemonSet launches the ovn ipsec networking components for all nodes.
    release.openshift.io/version: "{{.ReleaseVersion}}"
spec:
  selector:
    matchLabels:
      app: ovn-ipsec
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 10%
  template:
    metadata:
      annotations:
        target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        # prevent blocks when node critical pods get evicted prior to workloads
        cluster-autoscaler.kubernetes.io/enable-ds-eviction: "false"
      labels:
        app: ovn-ipsec
        component: network
        type: infra
        openshift.io/component: network
        kubernetes.io/os: "linux"
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: network.operator.openshift.io/dpu-host
                operator: DoesNotExist
      serviceAccountName: ovn-kubernetes-node
{{ if .IPsecServiceCheckOnHost }}
      hostPID: true
{{ end }}
      hostNetwork: true
      dnsPolicy: Default
      priorityClassName: "system-node-critical"
      initContainers:
      - name: ovn-keys
        image: "{{.OvnImage}}"
        command:
        - /bin/bash
        - -c
        - |
          #!/bin/bash
          set -exuo pipefail

{{ if .IPsecServiceCheckOnHost }}
          if chroot /proc/1/root systemctl is-active --quiet ipsec.service; then
            echo "host has ipsec.service running and therefore ipsec will be configured by ipsec host daemonset, this ovn ipsec container doesnt need to init anything"
            exit 0
          fi
{{ end }}

{{ if .NETWORK_NODE_IDENTITY_ENABLE }}
          # When NETWORK_NODE_IDENTITY_ENABLE is true, use the per-node certificate to create a kubeconfig
          # that will be used to talk to the API


          # Wait for cert file
          retries=0
          tries=20
          key_cert="/etc/ovn/ovnkube-node-certs/ovnkube-client-current.pem"
          while [ ! -f "${key_cert}" ]; do
            (( retries += 1 ))
            if [[ "${retries}" -gt ${tries} ]]; then
              echo "$(date -Iseconds) - ERROR - ${key_cert} not found"
              return 1
            fi
            sleep 1
          done

          cat << EOF > /var/run/ovnkube-kubeconfig
          apiVersion: v1
          clusters:
            - cluster:
                certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                server: {{.K8S_APISERVER}}
              name: default-cluster
          contexts:
            - context:
                cluster: default-cluster
                namespace: default
                user: default-auth
              name: default-context
          current-context: default-context
          kind: Config
          preferences: {}
          users:
            - name: default-auth
              user:
                client-certificate: /etc/ovn/ovnkube-node-certs/ovnkube-client-current.pem
                client-key: /etc/ovn/ovnkube-node-certs/ovnkube-client-current.pem
          EOF
          export KUBECONFIG=/var/run/ovnkube-kubeconfig
{{ end }}

          # Every time we restart this container, we will create a new key pair if
          # we are close to key expiration or if we do not already have a signed key pair.
          #
          # Each node has a key pair which is used by OVS to encrypt/decrypt/authenticate traffic
          # between each node. The CA cert is used as the root of trust for all certs so we need
          # the CA to sign our certificate signing requests with the CA private key. In this way,
          # we can validate that any signed certificates that we receive from other nodes are
          # authentic.
          echo "Configuring IPsec keys"

          cert_pem=/etc/openvswitch/keys/ipsec-cert.pem

          # If the certificate does not exist or it will expire in the next 6 months
          # (15770000 seconds), we will generate a new one.
          if ! openssl x509 -noout -dates -checkend 15770000 -in $cert_pem; then
            # We use the system-id as the CN for our certificate signing request. This
            # is a requirement by OVN.
            cn=$(ovs-vsctl --retry -t 60 get Open_vSwitch . external-ids:system-id | tr -d "\"")

            mkdir -p /etc/openvswitch/keys

            # Generate an SSL private key and use the key to create a certitificate signing request
            umask 077 && openssl genrsa -out /etc/openvswitch/keys/ipsec-privkey.pem 2048
            openssl req -new -text \
                        -extensions v3_req \
                        -addext "subjectAltName = DNS:${cn}" \
                        -subj "/C=US/O=ovnkubernetes/OU=kind/CN=${cn}" \
                        -key /etc/openvswitch/keys/ipsec-privkey.pem \
                        -out /etc/openvswitch/keys/ipsec-req.pem

            csr_64=$(base64 -w0 /etc/openvswitch/keys/ipsec-req.pem) # -w0 to avoid line-wrap

            # Request that our generated certificate signing request is
            # signed by the "network.openshift.io/signer" signer that is
            # implemented by the CNO signer controller. This will sign the
            # certificate signing request using the signer-ca which has been
            # set up by the OperatorPKI. In this way, we have a signed certificate
            # and our private key has remained private on this host.
            cat <<EOF | kubectl create -f -
            apiVersion: certificates.k8s.io/v1
            kind: CertificateSigningRequest
            metadata:
              generateName: ipsec-csr-$(hostname)-
              labels:
                k8s.ovn.org/ipsec-csr: $(hostname)
            spec:
              request: ${csr_64}
              signerName: network.openshift.io/signer
              usages:
              - ipsec tunnel
          EOF
            # Wait until the certificate signing request has been signed.
            counter=0
            until [ -n "$(kubectl get csr -lk8s.ovn.org/ipsec-csr="$(hostname)" --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-1:].status.certificate}' 2>/dev/null)" ]
            do
              counter=$((counter+1))
              sleep 1
              if [ $counter -gt 60 ];
              then
                      echo "Unable to sign certificate after $counter seconds"
                      exit 1
              fi
            done

            # Decode the signed certificate.
            kubectl get csr -lk8s.ovn.org/ipsec-csr="$(hostname)" --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-1:].status.certificate}' | base64 -d | openssl x509 -outform pem -text -out $cert_pem

            # Get the CA certificate so we can authenticate peer nodes.
            openssl x509 -in /signer-ca/ca-bundle.crt -outform pem -text -out /etc/openvswitch/keys/ipsec-cacert.pem
          fi

          # Configure OVS with the relevant keys for this node. This is required by ovs-monitor-ipsec.
          #
          # Updating the certificates does not need to be an atomic operation as
          # the will get read and loaded into NSS by the ovs-monitor-ipsec process
          # which has not started yet.
          ovs-vsctl --retry -t 60 set Open_vSwitch . other_config:certificate=$cert_pem \
                                                     other_config:private_key=/etc/openvswitch/keys/ipsec-privkey.pem \
                                                     other_config:ca_cert=/etc/openvswitch/keys/ipsec-cacert.pem
        env:
        - name: K8S_NODE
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        securityContext:
          privileged: true
        volumeMounts:
{{ if .NETWORK_NODE_IDENTITY_ENABLE }}
        - mountPath: /etc/ovn/
          name: etc-ovn
{{ end }}
        - mountPath: /var/run/openvswitch
          name: host-var-run-ovs
        - mountPath: /signer-ca
          name: signer-ca
        - mountPath: /etc/openvswitch
          name: etc-openvswitch
        resources:
          requests:
            cpu: 10m
            memory: 100Mi
        terminationMessagePolicy: FallbackToLogsOnError
      containers:
      # ovs-monitor-ipsec and libreswan daemons
      - name: ovn-ipsec
        image: "{{.OvnImage}}"
        command:
        - /bin/bash
        - -c
        - |
          #!/bin/bash
          set -exuo pipefail

          function cleanup()
          {
            # In order to maintain traffic flows during container restart, we
            # need to ensure that xfrm state and policies are not flushed.

            # Don't allow ovs monitor to cleanup persistent state
            kill "$(cat /var/run/openvswitch/ovs-monitor-ipsec.pid 2>/dev/null)" 2>/dev/null || true
            # Don't allow pluto to clear xfrm state and policies on exit
            kill -9 "$(cat /var/run/pluto/pluto.pid 2>/dev/null)" 2>/dev/null || true

            /usr/sbin/ipsec --stopnflog
            exit 0
          }
          trap cleanup SIGTERM

{{ if .IPsecServiceCheckOnHost }}
          if chroot /proc/1/root systemctl is-active --quiet ipsec.service; then
            echo "host has ipsec.service running and therefore ipsec will be configured by ipsec host daemonset, this ovn ipsec container will sleep to infinity"
            sleep infinity
          fi
{{ end }}

          # Don't start IPsec until ovnkube-node has finished setting up the node
          counter=0
          until [ -f /etc/cni/net.d/10-ovn-kubernetes.conf ]
          do
            counter=$((counter+1))
            sleep 1
            if [ $counter -gt 300 ];
            then
                    echo "ovnkube-node pod has not started after $counter seconds"
                    exit 1
            fi
          done
          echo "ovnkube-node has configured node."

          # After a restart of this container (or on initial startup), we flush xfrm state and policy
          # before we start pluto and ovs-monitor-ipsec in order to start in a known good state. This
          # will result in a small interruption in traffic until pluto and ovs-monitor-ipsec start again.
          ip x s flush
          ip x p flush

          # Workaround for https://github.com/libreswan/libreswan/issues/373
          ulimit -n 1024

          /usr/libexec/ipsec/addconn --config /etc/ipsec.conf --checkconfig
          # Check kernel modules only for libreswan version <= 5.2. The _stackmanager binary is
          # removed from 5.3 onwards, so this check is not needed on later versions.
          if [ -e /usr/libexec/ipsec/_stackmanager ]; then
            /usr/libexec/ipsec/_stackmanager start
          fi
          # Check nss database status
          /usr/sbin/ipsec --checknss
          # Start the pluto IKE daemon
          /usr/libexec/ipsec/pluto --leak-detective --config /etc/ipsec.conf --logfile /var/log/openvswitch/libreswan.log

          # Start ovs-monitor-ipsec which will monitor for changes in the ovs
          # tunnelling configuration (for example addition of a node) and configures
          # libreswan appropriately.
          # We are running this in the foreground so that the container will be restarted when ovs-monitor-ipsec fails.
          /usr/libexec/platform-python /usr/share/openvswitch/scripts/ovs-monitor-ipsec \
            --pidfile=/var/run/openvswitch/ovs-monitor-ipsec.pid --ike-daemon=libreswan --no-restart-ike-daemon \
            --ipsec-d /var/lib/ipsec/nss \
            --log-file --monitor unix:/var/run/openvswitch/db.sock
        env:
        - name: K8S_NODE
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        securityContext:
          privileged: true
        volumeMounts:
        # To check that network setup is complete
        - mountPath: /etc/cni/net.d
          name: host-cni-netd
        - mountPath: /var/run/openvswitch
          name: host-var-run-ovs
        - mountPath: /var/log/openvswitch/
          name: host-var-log-ovs
        - mountPath: /etc/openvswitch
          name: etc-openvswitch
        resources:
          requests:
            cpu: 10m
            memory: 100Mi
        terminationMessagePolicy: FallbackToLogsOnError
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - |
              #!/bin/bash
{{ if .IPsecServiceCheckOnHost }}
              if chroot /proc/1/root systemctl is-active --quiet ipsec.service; then
                echo "host has ipsec.service running and therefore ipsec will be configured by ipsec host daemonset, this ovn ipsec container is always \"alive\""
                exit 0
              fi
{{ end }}
              if [[ $(ipsec whack --trafficstatus | wc -l) -eq 0 ]]; then
                echo "no ipsec traffic configured"
                exit 10
              fi
          initialDelaySeconds: 15
          periodSeconds: 60
      nodeSelector:
        kubernetes.io/os: "linux"
      terminationGracePeriodSeconds: 10
      volumes:
{{ if .NETWORK_NODE_IDENTITY_ENABLE }}
      - name: etc-ovn
        hostPath:
          path: /var/lib/ovn-ic/etc
{{ end }}
      - name: host-var-log-ovs
        hostPath:
          path: /var/log/openvswitch
          type: DirectoryOrCreate
      - name: host-var-run-ovs
        hostPath:
          path: /var/run/openvswitch
          type: DirectoryOrCreate
      - name: signer-ca
        configMap:
          name: signer-ca
      - name: etc-openvswitch
        hostPath:
          path: /var/lib/openvswitch/etc
          type: DirectoryOrCreate
      - name: host-cni-netd
        hostPath:
          path: "{{.CNIConfDir}}"
      tolerations:
      - operator: "Exists"
{{end}}
