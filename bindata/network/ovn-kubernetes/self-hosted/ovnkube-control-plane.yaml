# The ovnkube control-plane components
kind: Deployment
apiVersion: apps/v1
metadata:
  name: ovnkube-control-plane
  namespace: openshift-ovn-kubernetes
  annotations:
    kubernetes.io/description: |
      This deployment launches the ovn-kubernetes controller (control-plane) networking components.
    release.openshift.io/version: "{{.ReleaseVersion}}"
spec:
  replicas: {{.ClusterManagerReplicas}}
  selector:
    matchLabels:
      app: ovnkube-control-plane
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 0
  template:
    metadata:
      annotations:
        target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
      labels:
        app: ovnkube-control-plane
        component: network
        type: infra
        openshift.io/component: network
        kubernetes.io/os: "linux"
    spec:
      serviceAccountName: ovn-kubernetes-control-plane
      hostNetwork: true
      dnsPolicy: Default
      priorityClassName: "system-cluster-critical"
      # volumes in all containers:
      # (container) -> (host)
      # /env -> configmap env-overrides - debug overrides
      containers:
      - name: kube-rbac-proxy
        image: {{.KubeRBACProxyImage}}
        command:
        - /bin/bash
        - -c
        - |
          #!/bin/bash
          set -euo pipefail
          TLS_PK=/etc/pki/tls/metrics-cert/tls.key
          TLS_CERT=/etc/pki/tls/metrics-cert/tls.crt
          # As the secret mount is optional we must wait for the files to be present.
          # The service is created in monitor.yaml and this is created in sdn.yaml.
          TS=$(date +%s)
          WARN_TS=$(( ${TS} + $(( 20 * 60)) ))
          HAS_LOGGED_INFO=0

          log_missing_certs(){
              CUR_TS=$(date +%s)
              if [[ "${CUR_TS}" -gt "WARN_TS"  ]]; then
                echo $(date -Iseconds) WARN: ovn-control-plane-metrics-cert not mounted after 20 minutes.
              elif [[ "${HAS_LOGGED_INFO}" -eq 0 ]] ; then
                echo $(date -Iseconds) INFO: ovn-control-plane-metrics-cert not mounted. Waiting 20 minutes.
                HAS_LOGGED_INFO=1
              fi
          }
          while [[ ! -f "${TLS_PK}" ||  ! -f "${TLS_CERT}" ]] ; do
            log_missing_certs
            sleep 5
          done

          echo $(date -Iseconds) INFO: ovn-control-plane-metrics-certs mounted, starting kube-rbac-proxy
          exec /usr/bin/kube-rbac-proxy \
            --logtostderr \
            --secure-listen-address=:9108 \
            --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 \
            --upstream=http://127.0.0.1:29108/ \
            --tls-private-key-file=${TLS_PK} \
            --tls-cert-file=${TLS_CERT}
        ports:
        - containerPort: 9108
          name: https
        resources:
          requests:
            cpu: 10m
            memory: 20Mi
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
        - name: ovn-control-plane-metrics-cert
          mountPath: /etc/pki/tls/metrics-cert
          readOnly: True
      # ovnkube-control-plane: central component that allocates IPAM for each node in the cluster
      - name: ovnkube-cluster-manager
        image: "{{.OvnControlPlaneImage}}"
        command:
        - /bin/bash
        - -c
        - |
          set -xe
{{- if .IsNetworkTypeLiveMigration }}
          echo "wait for node-subnets node annotation in live migration mode"
          # The node-subnets node annotation needs to be created according to the openshift-sdn hostsubnet CRs by ovnkube-node pods before running ovnkube-cluster-manager.
          while kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.annotations.k8s\.ovn\.org/node-subnets}{"\n"}{end}'| grep -v -q "default"; do echo "subnet annotation is missing for some nodes" && sleep 3; done
{{- end }}
          if [[ -f "/env/_master" ]]; then
            set -o allexport
            source "/env/_master"
            set +o allexport
          fi

          ovn_v4_join_subnet_opt=
          if [[ "{{.V4JoinSubnet}}" != "" ]]; then
            ovn_v4_join_subnet_opt="--gateway-v4-join-subnet {{.V4JoinSubnet}}"
          fi
          ovn_v6_join_subnet_opt=
          if [[ "{{.V6JoinSubnet}}" != "" ]]; then
            ovn_v6_join_subnet_opt="--gateway-v6-join-subnet {{.V6JoinSubnet}}"
          fi

          ovn_v4_transit_switch_subnet_opt=
          if [[ "{{.V4TransitSwitchSubnet}}" != "" ]]; then
            ovn_v4_transit_switch_subnet_opt="--cluster-manager-v4-transit-subnet {{.V4TransitSwitchSubnet}}"
          fi
          ovn_v6_transit_switch_subnet_opt=
          if [[ "{{.V6TransitSwitchSubnet}}" != "" ]]; then
            ovn_v6_transit_switch_subnet_opt="--cluster-manager-v6-transit-subnet {{.V6TransitSwitchSubnet}}"
          fi

          dns_name_resolver_enabled_flag=
          if [[ "{{.DNS_NAME_RESOLVER_ENABLE}}" == "true" ]]; then
            dns_name_resolver_enabled_flag="--enable-dns-name-resolver"
          fi

          persistent_ips_enabled_flag="--enable-persistent-ips"

          # This is needed so that converting clusters from GA to TP
          # will rollout control plane pods as well
          network_segmentation_enabled_flag=
          multi_network_enabled_flag=
          if [[ "{{.OVN_MULTI_NETWORK_ENABLE}}" == "true" ]]; then
            multi_network_enabled_flag="--enable-multi-network"
          fi
          if [[ "{{.OVN_NETWORK_SEGMENTATION_ENABLE}}" == "true" ]]; then
            if [[ "{{.OVN_MULTI_NETWORK_ENABLE}}" != "true" ]]; then
              multi_network_enabled_flag="--enable-multi-network"
            fi
            network_segmentation_enabled_flag="--enable-network-segmentation"
          fi

          route_advertisements_enable_flag=
          if [[ "{{.OVN_ROUTE_ADVERTISEMENTS_ENABLE}}" == "true" ]]; then
            route_advertisements_enable_flag="--enable-route-advertisements"
          fi
  
          preconfigured_udn_addresses_enable_flag=
          if [[ "{{.OVN_PRE_CONF_UDN_ADDR_ENABLE}}" == "true" ]]; then
            preconfigured_udn_addresses_enable_flag="--enable-preconfigured-udn-addresses"
          fi

          # Enable multi-network policy if configured (control-plane always full mode)
          multi_network_policy_enabled_flag=
          if [[ "{{.OVN_MULTI_NETWORK_POLICY_ENABLE}}" == "true" ]]; then
            multi_network_policy_enabled_flag="--enable-multi-networkpolicy"
          fi

          # Enable admin network policy if configured (control-plane always full mode)
          admin_network_policy_enabled_flag=
          if [[ "{{.OVN_ADMIN_NETWORK_POLICY_ENABLE}}" == "true" ]]; then
            admin_network_policy_enabled_flag="--enable-admin-network-policy"
          fi
          
          if [ "{{.OVN_GATEWAY_MODE}}" == "shared" ]; then
            gateway_mode_flags="--gateway-mode shared"
          elif [ "{{.OVN_GATEWAY_MODE}}" == "local" ]; then
            gateway_mode_flags="--gateway-mode local"
          else
            echo "Invalid OVN_GATEWAY_MODE: \"{{.OVN_GATEWAY_MODE}}\". Must be \"local\" or \"shared\"."
            exit 1
          fi

          echo "I$(date "+%m%d %H:%M:%S.%N") - ovnkube-control-plane - start ovnkube --init-cluster-manager ${K8S_NODE}"
          exec /usr/bin/ovnkube \
            --enable-interconnect \
            --init-cluster-manager "${K8S_NODE}" \
            --config-file=/run/ovnkube-config/ovnkube.conf \
            --loglevel "${OVN_KUBE_LOG_LEVEL}" \
            --metrics-bind-address "127.0.0.1:29108" \
            --metrics-enable-pprof \
            --metrics-enable-config-duration \
            ${ovn_v4_join_subnet_opt} \
            ${ovn_v6_join_subnet_opt} \
            ${ovn_v4_transit_switch_subnet_opt} \
            ${ovn_v6_transit_switch_subnet_opt} \
            ${dns_name_resolver_enabled_flag} \
            ${persistent_ips_enabled_flag} \
            ${multi_network_enabled_flag} \
            ${network_segmentation_enabled_flag} \
            ${gateway_mode_flags} \
            ${route_advertisements_enable_flag} \
            ${preconfigured_udn_addresses_enable_flag} \
            --enable-egress-ip=true \
            --enable-egress-firewall=true \
            --enable-egress-qos=true \
            --enable-egress-service=true \
            --enable-multicast \
            --enable-multi-external-gateway=true \
            ${multi_network_policy_enabled_flag} \
            ${admin_network_policy_enabled_flag}
        volumeMounts:
        - mountPath: /run/ovnkube-config/
          name: ovnkube-config
        - mountPath: /env
          name: env-overrides
        resources:
          requests:
            cpu: 10m
            memory: 300Mi
        env:
        - name: OVN_KUBE_LOG_LEVEL
          value: "4"
        - name: K8S_NODE
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME  # standalone cluster manager will read POD_NAME and use it as its identity for leader election
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        ports:
        - name: metrics-port
          containerPort: 29108
        terminationMessagePolicy: FallbackToLogsOnError
      nodeSelector:
        node-role.kubernetes.io/master: ""
        kubernetes.io/os: "linux"
      volumes:
      - name: ovnkube-config
        configMap:
          name: ovnkube-config
      - name: env-overrides
        configMap:
          name: env-overrides
          optional: true
      - name: ovn-control-plane-metrics-cert
        secret:
          secretName: ovn-control-plane-metrics-cert
          optional: true
      tolerations:
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
      - key: "node.kubernetes.io/not-ready"
        operator: "Exists"
      - key: "node.kubernetes.io/unreachable"
        operator: "Exists"
      - key: "node.kubernetes.io/network-unavailable"
        operator: "Exists"
